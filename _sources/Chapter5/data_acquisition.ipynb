{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8794e27-7581-48be-8c54-04d535c866f9",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b628c5d2-f36d-4301-8378-10a043c4a1ab",
   "metadata": {},
   "source": [
    "Time to talk about how we go about getting real data.  There's tons of different ways, and new ones being created all of the time.  These are by far the most popular for our domain.  Some require specialized infrastructures and environements, and others are great for learning and testing.  For our purposes, we're going to focus on the learning and testing aspects using real-world datasets publically available.\n",
    "\n",
    "We'll cover the following for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b2b7a-9548-4618-94ee-d42b9134409b",
   "metadata": {},
   "source": [
    "- Downloading files\n",
    "- Database/Warehouse\n",
    "- APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ac50e-86e4-4a3c-bc2a-307b339ec480",
   "metadata": {},
   "source": [
    "<h3>Downloading Files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bf188-5f42-4537-9774-936b9ff56f47",
   "metadata": {},
   "source": [
    "By far the most common way you'll get data is by simple downloading of your files, until you start dealing with more sophisticated production ready infrastructures in the real-world anyway.  There are tons of different file extensions you will run into in the wild so there's no hope we could be exhaustive here, but we can review a couple of the most common ones which should cover you for the majority of use cases.\n",
    "\n",
    "In practice, you'll occasionally run into new extensions you've never worked with before, and even errors running the most basic ingestion methods like `pd.read_csv()` due to encoding issues or unknown formatting problems in the file.  Python package documentation, <a href=\"https://stackoverflow.com/\">stackoverflow</a>, and Google will continue to be your best friends when you run into errors or new files types.  You can generally bet that someone else has run into the same problem you're having and that the Q&A is out on the internet somewhere.\n",
    "\n",
    "The workhorse of this acquisition group is overwhelmingly going to be our trusty <a href=\"https://pandas.pydata.org/docs/index.html\">Pandas</a> library.  It can be leveraged for a huge variety of different types of files and is extremely flexible.\n",
    "\n",
    "Off we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e88b9f-694e-4cbb-9563-f35b873b2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c4103-d730-40bd-83b4-6c0e181b89b9",
   "metadata": {},
   "source": [
    "<h5>CSV/Excel/TXT Files</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e2024-82f4-4263-8797-efc930688a27",
   "metadata": {},
   "source": [
    "Businesses run on Excel.  I can't say this enough.  It is the most common data storage and analytics tool for a reason.  It simply works and is super intuitive.  Excel puts some very sophisticated tools in the hands of non-technical resources, and it's the lowest and cheapest bar to entry for analytics.  Most companies will use it for these reasons, so skills are transferable from company to company and there's no lock out due to commercial licensing.\n",
    "\n",
    "We already saw this in the {doc}`../Chapter5/data_types` section, but let's again read in a CSV/Excel file from our Github storage.  To do so we'll use pandas `pd.read_csv()` method.  This is an _extremely_ common way to read in data as you'll quickly learn.  Here we're pointing to our Github repository, but you will most often have your files in local storage on your computer so you'll just point the URL path to your working directory or local file.  Simple as that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086e351-488d-4419-aad7-9a6f45a0540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Davis height/weight data from Github as a .csv file\n",
    "url = 'https://github.com/bradybr/practical-data-science-and-ml/blob/main/datasets/Davis.csv?raw=true'\n",
    "dat = pd.read_csv(url)\n",
    "dat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c9ad5-ed46-4101-b2ca-ae84408f35e2",
   "metadata": {},
   "source": [
    "There are a ton of parameters with this function, so definitely read the documentation and do some Googling when you run into any difficulties.  For example, let's see what happens if we read in our file as a \"tab-delimited\" text file.  To do that we'll use the separator argument `sep = \"\\t\"` in `pd.read_csv()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff055c23-970b-4304-a759-f80d5dd5a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Davis height/weight data from Github as a .txt file\n",
    "url = 'https://github.com/bradybr/practical-data-science-and-ml/blob/main/datasets/Davis.csv?raw=true'\n",
    "dat = pd.read_csv(url, sep = \"\\t\")\n",
    "dat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd10227-4c12-46ec-a089-9aa28337b0d6",
   "metadata": {},
   "source": [
    "Interesting.  It looks like we do need to understand if our file is comma, tab, semicolon, or otherwise separated.  If our file was semicolon delimited, we would have written `sep = \";\"`, but as it turns out, this one matched the default with comma separated so it was able to parse it out into a dataframe as-is.\n",
    "\n",
    "If you have a true .xls file instead of a .csv file, then check out `pd.read_excel()` function, which handles the specialities which come with the more complex file type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097b199-5ec2-45a5-b27f-53952bc98ce6",
   "metadata": {},
   "source": [
    "<h5>XML/HTML Tables</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8620e50-ad52-4f44-b411-8eedbb72ae69",
   "metadata": {},
   "source": [
    "Reading data from the internet can often be very finicky and challenging.  Technology often gets involved, and you also need to know a little bit about the language of the internet: HTML.  \n",
    "\n",
    "The most painful part is that a method which worked yesterday can easily break tomorrow.  Sites can change their HTML code anytime so you always have to stay on your toes.  It's also a burden on the sites resources you're trying to reach so it's quite common to have your IP address blocked if you abuse requesting information too often.  Specific types of sites are more sensitive to this than others, specifically ones that believe their data is a competitive advantage or proprietary in some way.  There are constant legal battles going on regarding whether or not the data belongs in the public domain after it's been published.\n",
    "\n",
    "To illustrate, we'll read in a standing table from Wikipedia that shouldn't give us any trouble.  The code below queries a specific Wikipedia page listing all of the public companies currently listed in the <a href=\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\">S&P500</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7835b370-f9f0-4ac7-9465-c2419b12ffe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Louisville, Kentucky</td>\n",
       "      <td>1997-10-06</td>\n",
       "      <td>1041061</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Electronic Equipment &amp; Instruments</td>\n",
       "      <td>Lincolnshire, Illinois</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>877212</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Warsaw, Indiana</td>\n",
       "      <td>2001-08-07</td>\n",
       "      <td>1136869</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZION</td>\n",
       "      <td>Zions Bancorporation</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Regional Banks</td>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>2001-06-22</td>\n",
       "      <td>109380</td>\n",
       "      <td>1873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Parsippany, New Jersey</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>1555280</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol              Security             GICS Sector  \\\n",
       "0      MMM                    3M             Industrials   \n",
       "1      AOS           A. O. Smith             Industrials   \n",
       "2      ABT                Abbott             Health Care   \n",
       "3     ABBV                AbbVie             Health Care   \n",
       "4      ACN             Accenture  Information Technology   \n",
       "..     ...                   ...                     ...   \n",
       "498    YUM           Yum! Brands  Consumer Discretionary   \n",
       "499   ZBRA    Zebra Technologies  Information Technology   \n",
       "500    ZBH         Zimmer Biomet             Health Care   \n",
       "501   ZION  Zions Bancorporation              Financials   \n",
       "502    ZTS                Zoetis             Health Care   \n",
       "\n",
       "                      GICS Sub-Industry    Headquarters Location  Date added  \\\n",
       "0              Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
       "1                     Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
       "2                 Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
       "3                         Biotechnology  North Chicago, Illinois  2012-12-31   \n",
       "4        IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
       "..                                  ...                      ...         ...   \n",
       "498                         Restaurants     Louisville, Kentucky  1997-10-06   \n",
       "499  Electronic Equipment & Instruments   Lincolnshire, Illinois  2019-12-23   \n",
       "500               Health Care Equipment          Warsaw, Indiana  2001-08-07   \n",
       "501                      Regional Banks     Salt Lake City, Utah  2001-06-22   \n",
       "502                     Pharmaceuticals   Parsippany, New Jersey  2013-06-21   \n",
       "\n",
       "         CIK      Founded  \n",
       "0      66740         1902  \n",
       "1      91142         1916  \n",
       "2       1800         1888  \n",
       "3    1551152  2013 (1888)  \n",
       "4    1467373         1989  \n",
       "..       ...          ...  \n",
       "498  1041061         1997  \n",
       "499   877212         1969  \n",
       "500  1136869         1927  \n",
       "501   109380         1873  \n",
       "502  1555280         1952  \n",
       "\n",
       "[503 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import certifi\n",
    "import urllib\n",
    "from io import StringIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# URL path\n",
    "url = (\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")\n",
    "\n",
    "# Open URL & parse table content\n",
    "html = urllib.request.urlopen(url, cafile = certifi.where())\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tables = soup.findAll(\"table\")\n",
    "\n",
    "# Convert to dataframe\n",
    "dat = pd.read_html(StringIO(str(tables[0])))[0]\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff09ecb-1ebd-48d3-b4cb-b3fa26af655e",
   "metadata": {},
   "source": [
    "This example is taking advantage of a known \"table\" structure on a webpage.  Often however you'll need parse an entire page and crawl through the HTML nodes by parent, children, and various tags to find exactly what you're looking for.  This is referred to as _web scraping_ and _parsing_.  You will definitely need to do some googling and researching a bit with lots of trail and error when you begin to work with scraping web data for real.\n",
    "\n",
    "There's much more you can do with the topic of web scrapping and accessing XML and HTML structures, but this is a bit beyond the scope of our lessons.  If you're interested, definitely check out Beautiful Soup, Selenium, and Scrappy libraries, just to name a few..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c0005-5491-44b4-a701-65936f0b297f",
   "metadata": {},
   "source": [
    "<h5>JSON Data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855cf42-a065-4163-956e-1302fd71a4e6",
   "metadata": {},
   "source": [
    "Closely coupled with internet data is the JSON structure type.  JSON stands for JavaScript Object Notation, and is a simple and convenient way to store and transport data in a text format.  You may run into this one with websites who choose to make their data publically available for download.  It's also very common to send and receive data this way in industrialized cloud infrastructures within large companies.\n",
    "\n",
    "Below we'll retrieve the <a href=\"https://www.eia.gov/outlooks/steo/data/browser/\">US Department of Energy's Short-Term Engergy Outlook</a> for Petroleum using their API call which returns data in a JSON format.  You'll need to sign up for your own API Key password if you'd like to try and replicate yourself.\n",
    "\n",
    "Notice how the data returned in a format that looks very much like a Python dictionary?  Pretty cool, huh?  The JSON format is very close to native Python language and should hopefully be pretty easy to deal with after you've access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b577a3-f386-4f0c-a4c8-6411fcf7cf94",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pw = '73aa6102f0b10664045a49a5f9789e24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b187a0-e292-43fd-92d1-810489fdddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"https://api.eia.gov/v2/petroleum/move/wkly/data/?frequency=weekly&data[0]=value&facets[series][]=WCREXUS2&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=\" + pw + \"&start=2024-01-01\"\n",
    "json.loads(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b75d67-d9a0-455e-9dae-fd2f34c73e64",
   "metadata": {},
   "source": [
    "<h5>Database/Warehouse Data</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68e2c2-4378-4f6b-a3a6-535f70957638",
   "metadata": {},
   "source": [
    "Retrieving data stored in some kind of structured relational database is also extremely common.  You may have a free version like MySQL setup for your personal use, or maybe a more powerful commerical cloud based offering like Microsoft Azure and Data Lake.  This is a deep and specific subject due to the techologies, installations, and authorizations required, which are also a bit outside of the scope for these lessons.\n",
    "\n",
    "Since we'll be dealing mostly with downloading files or accessing data via Python libraries, we'll leave this as an exercise for the reader to investigate further in his or her own time.  Below is the link to the popular and free MySQL offering where you can read more about connecting with Python.\n",
    "\n",
    "And remember when we suggested learning the SQL language might be a good thing when discussing programming languages?  This is where it will come into play.  These relational databases are extremely common for warehousing enterprise data, and SQL is by far the most popular programming language to work with these data repositories.\n",
    "\n",
    "<a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-introduction.html\">MySQL Python Connector</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a3f2d-9eef-4ef7-bcfc-f6fc785e2e54",
   "metadata": {},
   "source": [
    "```{figure} ../images/mysql_connector.png\n",
    "---\n",
    "width: 900px\n",
    "name: mysql-connector-fig\n",
    "---\n",
    "MySQL Python Connector Introduction documentation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9f49d-ba4c-4984-b5e8-873f8a637aad",
   "metadata": {},
   "source": [
    "<h5>APIs</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd084a-cb14-4bd6-8116-fc448a2fb935",
   "metadata": {},
   "source": [
    "We've actually just seen an example of an API call to retrieve data from the Department of Energy.  API stands for Application Programming Interface, and are standard protocols which allow programs and applications to communicate with each other.  They are essentially end-to-end connection points that have been set up to faciliate the transfer of data using request calls and responses.\n",
    "\n",
    "The DOE call above is just one example, however API's have become very common with the explosion of business monetization of data as a core competency.  Social media sites like Twitter, Facebook, and the like, as well as other companies working with popular consumable data such as weather, stock prices, etc. are commonly utilizing this technology today.  Companies typically open these up to what are known as the \"Developer\" community.  You can sign up for an account and receive API Keys and authentication tokens which allow you to begin accessing data straight away.  Because this data is often a revenue stream for the provider, unfortunatly you will frequently run into a paywall for good data though.  Hard to blame them, seeing as they've incurred all of the expenses towards collecting and hosting the data, so it does seem only fair that they're compensated for it somehow.\n",
    "\n",
    "We won't go into any more detail here since we've already shown an example and due to needing to create developer profiles.  Our work for this course will be mostly with downloaded files or through Python packages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
