{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8794e27-7581-48be-8c54-04d535c866f9",
   "metadata": {},
   "source": [
    "# Data Charactersitics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31120e0f-a34d-4b3c-ac33-102c75aa1311",
   "metadata": {},
   "source": [
    "I wish I could tell you that your data will always be clean, complete, and ready for analysis.  I really do.  The truth however is far from it.  Real world data is messy.  Even working for a $100 Bn company, I'm continually shocked at how difficult it is to create, standardize, archive, and retrieve good data.  Very rarely does data come ready to go for proof of concept analytics.  It's only after we go through the effort to identify, extract, and play with and massage the data to prove there's some value in the solution we're building, that we then go to the next step of building a standard data asset.  This new data asset will now deliver the data in exactly the way we need it for use in our production program from now on.  But we have to get there first through exploratory analysis.\n",
    "\n",
    "There's no end to the unique challenges you'll run into once you start working with data, and again there's no standardization from dataset to dataset.  What we can do though is lay out a normal workflow that will always be a good idea to start with, independent of any differences across the different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d20729-e708-42ad-90c8-01b0cee31849",
   "metadata": {},
   "source": [
    "We'll get some overlap between this section with the integrity and plotting sections to follow, but don't worry if you see us plotting something, calling out an issue or fixing something and you don't fully understand it yet.  It may make more sense to cover a new topic in a next section if we don't fully explain it here.  Wee have to start somewhere though, so let's get to it.\n",
    "\n",
    "Here we'll go through the following in this lesson.\n",
    "\n",
    "- Data shape & features\n",
    "- Descriptive summaries\n",
    "- Missing values/records\n",
    "- Duplicates\n",
    "- Data Quality Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93149495-b7f5-452a-a21b-4dc592b53155",
   "metadata": {},
   "source": [
    "<h3>Data Shape & Features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb58981-3a36-4bfc-ab75-104eeff52579",
   "metadata": {},
   "source": [
    "Almost without exception, the first thing you should do is simply take a look at your data.  Read it in and then print it to the screen, or view it through an object explorer.  Just see what you have before doing anything else.\n",
    "\n",
    "Does it look clean?  Is it human interpretable?  Do you understand what all of the columns mean?  Do you need to ask the business domain sponsor for any explanations or data dictionaries for special keys or encodings?  What about the unit of analysis?  Can you tell what it is?\n",
    "\n",
    "Let's read in a dataset and see all of these in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c5da9-94f9-4d77-a8ff-d5402eb95f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7b68b-ce4d-4d5c-b05c-820a4d9c4aa2",
   "metadata": {},
   "source": [
    "The data we're going to play with is a large sleep study tracking the average minutes per night for various sleep patterns, over a one week period of time, for 877 participants.  The recorded sleep states are as follows.\n",
    "\n",
    "- active_mins = not asleep\n",
    "- sleep_disturb_mins =  sleeping, but not in a REM cycle\n",
    "- sleep_rem_mins = sleeping, REM cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fac6a1-b978-4ad0-851a-b04173ad625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from github repository\n",
    "url = 'https://github.com/bradybr/practical-data-science-and-ml/blob/main/datasets/sleep_study.csv?raw=true'\n",
    "dat = pd.read_csv(url, sep = ',')\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ee970-6832-4e52-b307-bc92d0ca7707",
   "metadata": {},
   "source": [
    "Can you infer the _unit of analysis_?  My vote would be-\n",
    "\n",
    "> Sleep pattern minutes, by participant (id)\n",
    "\n",
    "Looks like there's one person per row as our level of analysis.  This would be cross-sectional data if we think back to our review of data types.  Other than that, it looks pretty clean and easy to understand.  Each person should have a unique \"id\" that identifies them, then some demographic details, and finally their average minutes for each sleep state during the study.  Easy enough.\n",
    "\n",
    "The dataframe print out shows us the number of rows (observations) and columns (features/variables), but we can also print these out by using the `.shape()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356fbc4-eb77-43e6-a43a-2f84c861feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc360ce0-511d-412c-8bf8-dd887eb5d450",
   "metadata": {},
   "source": [
    "Cool.  They match up!\n",
    "\n",
    "Generally after looking at my data, the next thing I want to consider are the feature data types.  Meaning, do my variables have the correct data types?  This matters because we will soon start to perform operations on our data and they'll need to be correct.  You can't perform mathematical operations on words, and you don't want to treat numeric variables as objects, so we'll need to cast them into the correct type if they were read-in incorrectly.\n",
    "\n",
    "So where do we start?  You guessed it.  There's a function for this too - `.info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d8768-e5e6-47e0-ae5d-2cb4ff6a4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf9f93-0bf6-4389-bed5-cac631e5b49d",
   "metadata": {},
   "source": [
    "Above we can see that the \"id\" feature is an integer.  Does that feel correct?  It shouldn't, because numbers carry implied characteristics.  Is id = 4, two times greater than id = 2?  Hopefully you said no.  These are not really numeric values.  They're just numbers being used as categorical identifiers for the participants in the study, so we want to recast \"id\" as an \"object\" (categorical string).  See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7715420-0df2-469a-8752-8bc28350d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the features you want to change and recast them using \".astype()\"\n",
    "vars = ['id']\n",
    "dat[vars] = dat[vars].astype('object')\n",
    "dat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e61096-8c32-44db-811a-c640f0ae5728",
   "metadata": {},
   "source": [
    "Great!  Now they all look correct.  The categorical object variables (i.e. id, gender, country) are all objects, and the numeric values (i.e. age, minutes) are all numeric floats.  Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222d0cf-1d07-48f6-a7a4-b238a4eafe28",
   "metadata": {},
   "source": [
    "<h3>Descriptive Summaries</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb988c-49b0-43a4-8996-63b5d0a35915",
   "metadata": {},
   "source": [
    "Next up, let's take a look at some summaries of our data.  We're generally making a first pass here trying to get a sense for what we're working with and if there might be some issues to address.  Try the list below to observe and see if they pass the sniff test or seem funny to you.\n",
    "\n",
    "- Numeric ranges (min, max, central tendencies, distribution shapes - skew/kurtosis)\n",
    "- Nonsensical values\n",
    "- Cardinality (number of unique values)\n",
    "- High/low uniqueness\n",
    "\n",
    "The easiest place to start is with the `.describe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7b0c1-568c-4bd0-b7b8-a9022c7098ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print numeric summary stats\n",
    "dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d6587-0bda-497b-bcb9-c0620664d837",
   "metadata": {},
   "source": [
    "Notice how we only see the numeric features?  This is because you can't get an averge or minimum value for a categorical string.  This is why it's so important to get your data types right.  If you don't, you'll end up with summaries that don't make any sense.  While we will usually go the next step and plot these distributions as well, this nice little table summary is really a succint way to spot anomalies.\n",
    "\n",
    "Do you notice anything odd in the table above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb392c-7237-4ebd-99cc-cd918353464d",
   "metadata": {},
   "source": [
    "How about a the minimum and maximum values for Age?  It's not possible to have anyone in the study with an age of 0, and it's just as unlikely to have someone at the age of 146.  So right away we know we have some data integrity issues going on that we'll need to solve for in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a0544-2352-433b-a6bc-f572320f5064",
   "metadata": {},
   "source": [
    "A couple of other things to take note of here as well:\n",
    "\n",
    "- Counts are all less than the total number of observations in the data (877), indicating we have missing records\n",
    "- Minimum values of 2 across the sleep pattern minutes looks suspicous\n",
    "- Likely placeholder '9999' values in the minute features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee4bb2-c2ea-4efb-9c85-1bdd3e2ccfd3",
   "metadata": {},
   "source": [
    "And the same thing below with the categorical features.  We have some missing values to deal with, but otherwise, the unique counts and top mode values make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b2f9-4734-4093-8c93-66ab95f31e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categorical summary stats\n",
    "dat[['id','gender','country','study_begin','study_end']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6663854-8fbd-4131-9846-f95df10860f2",
   "metadata": {},
   "source": [
    "Did you notice that the study begin date only has one unique static value of 4/3/2021?  This is both confirmation that everyone started on the same date and we have no issues, and also telling us that the variable will serve no further purpose for us since there is no variability in the data.  Variables that do not change at all carry no information content so they can usually be removed from our analysis.\n",
    "\n",
    "Did you also notice there are three unique values for the \"study_end\" date?  There should only be one since the study ended after one week for all participants.  Let's take a closer look by printing a frequency table using the `.value_counts()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabab545-f8fa-4912-b76c-4a2bb064592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency table for \"study_end\" date\n",
    "dat.study_end.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb40b50-5a55-4d96-ae87-1d986beb102e",
   "metadata": {},
   "source": [
    "Interesting.  We have two that closed out a few days after the study ended on 4/9/23, and then one around six months later.  No idea what that one's about.  This is an example of when we'd go back to our business domain expert to find out how this might of happened.  Were they typos?  Or maybe some reason two were recorded three days after the study closed, and then someone entered that date instead of when the study actually ended?  And what about the fact that we only have 861 recorded end dates?  What about the other 16 participants?  Did they drop out of the study early or were these just omissions?\n",
    "\n",
    "So many questions that we'd need to follow up on so we can figure out how to handle them.  Stay tuned for the next {doc}`../Chapter5/data_integrity` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264bbf64-7fca-49f4-842a-88e63278b696",
   "metadata": {},
   "source": [
    "<h3>Missing Values/Records</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0c353-4ced-49ce-a35e-269b1dbbb6be",
   "metadata": {},
   "source": [
    "The topic of missing values could fill the contents of a book by itself believe it or not.  There are several different kinds of \"missing\" values, and also several different mechanisms that create missing values which all have implications for the different ways we need to handle them.\n",
    "\n",
    "At the end of the day, it's always a battle between removing the entire variable (column) with the missing values, removing any observation (row) entirely that has missing values, or _imputing_ (filling in) any missing values with a proxy value so we're are able to keep the observation and still use it in our analysis.  The main considerations with imputation are, 1) what methods are we going to use to do so, and 2) what percentage of imputed values are too many that render the variable too synthetic?  If you impute and fill in 90% of a column of data with approximated values, is it really going to tell you anything worthwhile?  As a general rule of thumb, you would probably be wise to remove any variable feature that has more than 50% of its values missing.  We'll address these considerations more in the next section.\n",
    "\n",
    "Now, what about the types of missing values?\n",
    "\n",
    "1. Missing Completely At Random (MCAR)\n",
    "\n",
    "   MCAR assumes that all of the missing values have the same probability of being absent, and there's no systematic bias or pattern as to how the values are missing.  It is best safely thought of as\n",
    "   unrealistic in the real-world.\n",
    "   \n",
    "3. Missing At Random (MAR)\n",
    "\n",
    "   MAR indicate that the probability of the value being missing is somehow related to the value of another observed feature(s) in the dataset.  This type may or may not introduce bias into the system.\n",
    "\n",
    "3. Missing Not At Random (MNAR)\n",
    "\n",
    "   MNAR missing values are missing because they are systematically related to the unobserved data in some way, i.e. related to factors outside of our controls which are not measured.  This type of\n",
    "   missing value will most likely introduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdce5a6-e2d8-47d5-bcb7-0397687ea372",
   "metadata": {},
   "source": [
    "Let's see what we have in our dataset by running the `is.na()` and summing up all of the missing values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d962677-85b6-4de4-9590-5c94155d8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NA's by feature\n",
    "dat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d58089-7ace-46f9-9e59-def05a46d2eb",
   "metadata": {},
   "source": [
    "We will have to deal with these in the next section when we talk to our business domain expert, but for now think about what types of missing values these might be.  The missing Age value may just be an omission because it's only 1 person, or it might be MNAR because it could be related to something outside of what we can observe.  If there were more, it might be MAR due to some kind of relation between maybe gender at a prefence to not give their age, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509abac-1d4c-415f-8717-1598b06a9697",
   "metadata": {},
   "source": [
    "One last consideration here that we'll cover when we get to the time series section under machine learning, is the situation of missing time periods.  It's quite common to see missing date records (rows) either due to omission, or the compression that happens in data storage when there is no value to record.  This will throw a wrench in the works for our time series algorithms that expect every single time period accounted for in our total range of dates.  This is known as _regular_ periodicity where the records are recorded at regular intervals, as opposed to irregular periodicity.  More to come on this topic later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b06669-8761-41b1-afc9-d2038581b917",
   "metadata": {},
   "source": [
    "<h3>Duplicates</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c99ef-0685-4f51-9c0e-0bbf45290acb",
   "metadata": {},
   "source": [
    "The last issue we may want to understand is the possibility of duplicates.  Luckily there's a built-in function for this one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cbf31-f6e5-42ec-9be3-de105728c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate observations\n",
    "dat[dat.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4536c3f-1d05-46e9-9402-5b65c1b05217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all of the duplicates by the id's identified above if you want to see all of them plus the originals\n",
    "dat[dat['id'].isin([89,238])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f4c64-f077-46d4-b907-92af8c8e267b",
   "metadata": {},
   "source": [
    "We'll address these in the next section too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b2088-25fa-4aaa-aa3b-381447451d4f",
   "metadata": {},
   "source": [
    "This can get quite a bit more complicated for different kinds of data, think panel data, where we have multiple levels and time periods in the data.  We'll likely need to use a \"split-apply-combine\" methodology with something like the pandas `.groupby().apply()` functionality.  More on this when we get to the {doc}`../Chapter5/wrangling` section soon.\n",
    "\n",
    "Ok, so that's it for all I would probably do at this time for my initial pass through of the data.  Coming up we will discuss how to fix all of these issues in the next section, and also start to introduce some basic plotting and graphing techinques you can use to explore the relationships in your data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f70bb6-883c-413d-b0e8-baf06fab5bea",
   "metadata": {},
   "source": [
    "The last topic I'd like to introduce in this section is the notion of a Data Quality Report, or Profile Report.  Learning how to work through all of this manually is extremely valuable time well spent.  If you can learn to think through how to identify issues, and think through the questions you want to ask of the data, you'll be a much stronger analyst for it.  Having said all of that, you should know that there are plenty of Python libraries we can import that essentially do everything we've just done automatically.\n",
    "\n",
    "Don't be angry.  Remember, you're a better analyst now.\n",
    "\n",
    "For kicks let's check one such automated example and see if we like it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a7e4d-032f-4187-ac84-4bd5be066d42",
   "metadata": {},
   "source": [
    "<h3>Data Quality Report</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89f929-69bc-4e8f-bbca-ffe49e9a12a5",
   "metadata": {},
   "source": [
    "I personally find the value in doing it all manually like we've done above, but I certainly understand the appeal of a simple two lines of code approach as well.  To each his or her own.  Use whatever suits your style.    \n",
    "\n",
    "See the `ydata_profiling` example below.  Give it a spin and see what you think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd7fa8-46bf-40f6-83eb-30b885b6409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "dqr = ProfileReport(dat, title = \"Profiling Report\")\n",
    "dqr.to_notebook_iframe()\n",
    "#dqr.to_file(\"dqr_report.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
