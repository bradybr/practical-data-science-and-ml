{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8794e27-7581-48be-8c54-04d535c866f9",
   "metadata": {},
   "source": [
    "# Data Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940e01f-be94-44ba-8a34-dfbc9db8cb4d",
   "metadata": {},
   "source": [
    "Coming out of the last section where we began showing some of the data challenges you'll run across, and hopefully you're starting to understand it's not just _read in the data and get to modeling_.  There's so much time devoted to getting the data, checking it, summarizing, asking questions, fixing issues, then rinse and repeat.  Real-world data is super messy and there's no one size fits all methodology.  Nearly every dataset will be unique and have some kind of peculiarities specific to the domain and data collections method.\n",
    "\n",
    "Remember talking about \"garbage in, garbage out\"?  If your data is riddled with holes and untrustworthy data, then your entire model and project won't be worth a darn.  So again, be sure to give this EDA step all the attention it requires.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012ad73-5e79-4a73-9742-14ccd1c43810",
   "metadata": {},
   "source": [
    "Since you've already seen how to identify most of these issues, we're going to jump straight into addressing them.  In this section we'll cover the most common issues you'll run across.  There are others to be sure, but they'll be less common and more domain specific in most cases.\n",
    "\n",
    "- Duplicate records\n",
    "- Missing values/records\n",
    "- Anomalous values/Outliers\n",
    "- Sensoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2531f52-7c41-4303-b3a8-c48ef7552a69",
   "metadata": {},
   "source": [
    "Below we'll speed up getting our data back into our environment and ready to tackle the integrity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff7aa7-eafe-4b23-b8be-696493de43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as gg\n",
    "\n",
    "# Read in the data from github repository\n",
    "url = 'https://github.com/bradybr/practical-data-science-and-ml/blob/main/datasets/sleep_study.csv?raw=true'\n",
    "dat = pd.read_csv(url, sep = ',')\n",
    "\n",
    "# Create a list of the features you want to change and recast them using \".astype()\"\n",
    "vars = ['id']\n",
    "dat[vars] = dat[vars].astype('object')\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32acd03f-2d2b-4750-909b-3ef3c881a775",
   "metadata": {},
   "source": [
    "<h3>Duplicate Records</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb942b40-7c51-45a2-90a4-6b62d7a2097e",
   "metadata": {},
   "source": [
    "Let's deal with the easiest ones first.  Remember we had a few duplicate observations?  Well, we spoke with our business expert, and she said there's no reason for those to exist and they are in fact real duplicate records.  We can go ahead and delete them.  \n",
    "\n",
    "Easy enough.  Let's find them again and then we'll use the `.drop_duplicates()` function to drop them from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8017fd7-eca5-4f45-8b31-2c8dd321fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate observations\n",
    "dat[dat.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc98f2-16b5-497f-88ce-e48bed604abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the duplicate records & check the dimensions of the dataset\n",
    "dat.drop_duplicates(inplace = True)\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d4d4e-9bc0-4bc9-8ed2-1d4cd56fa858",
   "metadata": {},
   "source": [
    "Success!  We only lost two of our observations and we're down to 875 records.  On to the missing values!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cf9cb-743f-464f-9bf3-ab79a3eab970",
   "metadata": {},
   "source": [
    "<h3>Missing Values/Records</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dac518-2d26-438f-9cce-92a5792cbfe8",
   "metadata": {},
   "source": [
    "Ok, so we have some missing values to deal with now.  Let's deal with the easy ones first.  We again asked our business domain expert to help us understand the data collection and participant intake process to reign in how these might have occurred.  Here's what we found out.\n",
    "\n",
    "- __Age:__  Participants self reported their age on their intake applications when applying for the study.  Any missing values were not followed up on when the study actually started.\n",
    "- __Study End Date:__ All dates should be 4/9/2023.  Any date after 4/9/2023 is invalid and a data entry mistake.\n",
    "- __Sleep Pattern Minutes:__  Missing values in sleep recordings were accidental technician omissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22141e-3513-4f1d-a560-3460abc0b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NA's by feature\n",
    "dat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f9acd-2c24-4f0b-88b1-af78308c0452",
   "metadata": {},
   "source": [
    "Age is an easy decision to remove for me.  It's just one missing value.  Yes we'll be losing a little information on the sleep variables side, but I can live with losing just one person.  If I were in doubt, it's possible to run a machine learning operation or some hypothesis tests to see if we could infer whether this person is more likely to be a male or female.  We know there are nearly 70% females in the study, so that right there could be our best guess at the moment if we wanted to impute the value so we could keep the participant.\n",
    "\n",
    "Yes it's only one person so it's not likely to hurt much if we get it wrong, but _because_ it's only one person is exactly the reason I'm going to go ahead and remove the observation.  For me, it's not worth the possibility of introducing _any_ potential for bias if we get it wrong, just for the benefit of having only one more observation.\n",
    "\n",
    "So let's go with this logic and remove the one observation missing the participant's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe737a-09d6-4990-9d0d-0e78a217dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the 1 observation with a missing age\n",
    "dat.dropna(subset = ['age'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942735b7-739f-4608-9355-d7230afc4d0d",
   "metadata": {},
   "source": [
    "Study End Data is an easy one too.  The business sponsor told us it should be the same date for all observations.  So let's set it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182aaa8-9000-4399-bd87-84436e9f5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all of the missing end dates to '4/9/2023'\n",
    "dat['study_end'] = '4/9/2023'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230879b-aebd-4636-bdeb-ecc781e04d9b",
   "metadata": {},
   "source": [
    "Now, for the missing sleep pattern mintues... There are few enough that we could just delete them and it probably wouldn't materially change anything in our analysis, hopefully.  On the other hand, it is just a few of them so we could take a shot at imputation because we would not be creating a large number of artificial values.  For this example it's probably a toss up as to whether or not it matters either way; however, you will definitely see more complicated and difficult decisions in the real world.\n",
    "\n",
    "When you're considering deleting information, which should be a last resort, you should get in the habit of understanding what you're deleting.  For example, you may be removing observations that are unique values in other features and you'd be losing visibility to this group entirely, or maybe they are very important interactions somewhere else in the values under study.\n",
    "\n",
    "Let's print all of the rows with any missing values so we can see what we're really talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b176279-8b31-4f7e-a3be-408c81d2d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all rows with any NA/NaN's\n",
    "dat[dat.isna().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a55a4-f8fe-4787-b774-dbdf4964b521",
   "metadata": {},
   "source": [
    "Hmm... we can see some observations are missing all three values, some are missing two, and some just one.  There doesn't seem to be any rhyme or reason to it.\n",
    "\n",
    "Maybe there's a pattern by gender that we could exploit.  If there's some diffence between the two genders, then we could simply use the mean or median values of each group to impute.  We'll get to plotting in the next section, but let's jump ahead a bit and see if we can visually spot any differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68885fc7-1d7a-4a4b-833b-81149ebb3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into a stacked-long format\n",
    "plt_dat = pd.melt(dat, id_vars = ['id','gender', 'age','country'])\n",
    "\n",
    "# Filter to only the minute columns & update the value column from an object to a float\n",
    "plt_dat = plt_dat[plt_dat.variable.isin(['active_mins','sleep_disturb_mins','sleep_rem_mins'])]\n",
    "plt_dat['value'] = plt_dat['value'].astype('float')\n",
    "\n",
    "# Remove the extreme age and minute outliers (we'll deal with them next)\n",
    "plt_dat = plt_dat[(plt_dat.age < 100) & (plt_dat.value < 2500)]\n",
    "\n",
    "# Plot the density distributions by Gender\n",
    "(gg.ggplot(plt_dat, gg.aes(x = 'value', color = 'variable')) +\n",
    "     gg.geom_density() +\n",
    "     gg.xlab('Minutes') +\n",
    "     gg.ylab('Density') +\n",
    "     gg.ggtitle('Average Sleep Minutes by Gender') +    \n",
    "     gg.facet_wrap('gender', ncol = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854296f-8907-4138-89b7-b83412094375",
   "metadata": {},
   "source": [
    "Interesting.  There are actually some minor and subtle, but noticeable, differences between the two genders if we look at the figure above.  For starters, the men were definitely more restless as observed by their flatter \"active_mins\" distribution, plus a flatter and almost multimodal distribution for \"disturbed sleep\", compared to the women's group.  The differences in their REM sleep are minor, and would probably call for a bit more rigorous hypothesis testing.\n",
    "\n",
    "Next, we check to see if they actual medians support our visual findings by calculating the overall medians compared to the medians for each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c11d05-4cd1-41eb-bf11-160e70d773f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the overall median for each sleep pattern\n",
    "overall = (dat.groupby(lambda x: True)[['active_mins', 'sleep_disturb_mins', 'sleep_rem_mins']]\n",
    "                                           .median()\n",
    "                                           .rename(index = {True: 'overall'}))\n",
    "\n",
    "#Calculate the overall median for each sleep pattern, by gender\n",
    "gender = (dat.groupby('gender')[['active_mins', 'sleep_disturb_mins', 'sleep_rem_mins']]\n",
    "                                  .median()\n",
    "                                  .reset_index()\n",
    "                                  .set_index('gender')\n",
    "                                  .rename_axis(None, axis = 'index'))\n",
    "\n",
    "# Concatenate the overall and by-gender medians together\n",
    "pd.concat([overall, gender])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb2214-9364-4013-8a3d-3f8e2d3ebb6c",
   "metadata": {},
   "source": [
    "Huh.  How about that?  There absolutely is a noticeable difference if we would have used the overall vs. the by-gender median.  Whether or not it's a statistically significant difference is another matter entirely though... more on this topic later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a239741-40d7-4076-aa7b-c785bd5a23cb",
   "metadata": {},
   "source": [
    "For our purposes now, this is enough to conclude that a simple way to fill in the missing sleep pattern minutes would be to calculate the median values for each gender group, and then impute those values into the missing observations. I'm going to use the median instead of mean here because we know it's robust to outliers, whereas using the mean would be significantly skewed by our extreme values.\n",
    "\n",
    "We could also have chosen to deal with our extreme values first, prior to dealing with imputations.  There's no right or wrong way, and I'll frequent flip the order of procedures depending on whatever seems to make sense for the data at hand.  My general preference is to start by removing anything I can, rows or columns, as soon as possible.  It'll only get in the way and cloud your EDA work so the faster you can get rid of useless data the better.\n",
    "\n",
    "Let's try our median-gender group imputation method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cfcbd-cf09-4832-8faf-623569d7445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the median value for all sleep minute NAs using the median value for each gender group\n",
    "vars = ['active_mins', 'sleep_disturb_mins', 'sleep_rem_mins']\n",
    "dat[vars] = dat[vars].fillna(dat.groupby('gender')[vars].transform('median'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a92ba-d7ed-4b4f-8e6b-37905b9ae50d",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "And that should be it.  If we re-run our `is.na().sum()` counts we should see the fruits of our labor with all of the missing values taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f540fe0-e0df-42eb-a9e5-4099efc4fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NA's by feature\n",
    "dat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc1190-b83a-4b20-9d18-650c5843df53",
   "metadata": {},
   "source": [
    "There's no end to the various methods available for imputing missing information, from very simple, to unnecessarily complex.  What we did is somewhere in the middle.  We could have also leveraged add-on Python packages that run machine learning models to predict missing values, or we could have tried some kind of unsupervised similarity methodology to see, from a mathematical point of view, which other participants look most like ours with the missing information.  No matter which you choose, they generally all do the same thing: Learning or estimating from the information available in the dataset in some way, and then generalizing and projecting those learnings onto the missing observations. \n",
    "\n",
    "In my experience though, most often it's not really worth the effort to go to super complicated lengths to impute values.  It ends up either going too far and introducing harmful bias to our analysis, or we could have simply removed the observation(s) without losing meaningful information, or it was complete overkill because a simple method would have worked just fine.  At the end of the day, you'll have to find what works for you in a given situation.\n",
    "\n",
    "```{tip}\n",
    "Try not to get into the habit of simply deleting data or imputing without analyzing a bit first and thinking critically about the implications.  You run the risk of losing valuable information on one hand, versus watering down your analysis on the other.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cc044-cf7f-4583-9a75-316145fe75fd",
   "metadata": {},
   "source": [
    "<h3>Anomalous Values/Outliers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28399f93-1494-42ac-a5f1-13825ef96418",
   "metadata": {},
   "source": [
    "Now on to the last general step of dealing with extreme values before I would start thinking about investigating variable relationships, information content, and much more robust analysis.  Extreme values are problematic for several reasons.  First, they may seriously distort the range of values for a feature, thus likely skewing any analysis we perform.  Secondly, many of the algorithms we're going to use will give undue weight to values that are more extreme.\n",
    "\n",
    "This is a tough one because there's often very interesting information in the tails of our distributions so we'd rather not willy nilly delete everything that seems extreme; however, we need to be very careful that we don't let these outliers bias our models.\n",
    "\n",
    "Ideally we'd love for all of our data features to be i.i.d. from a standard normal distribution, although we shouldn't hold our breath.  The best we can hope for is that we tamp down the values that will unnecessarily distort our analysis without losing too much interesting information in the process, which will ultimately give our models the best chance of finding a viable solution.\n",
    "\n",
    "Where do we start then?  Maybe we should start with a general understanding of what are considered \"outliers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a21e2-eb17-4ec4-8c25-dbbbce89f096",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Generally an outlier is consider to be 1.5x +/- the inner quartile range (IQR) of a feature.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276fef8-f71b-4f1c-b91e-01583168ce24",
   "metadata": {},
   "source": [
    "What's an inner quartile range you're asking?  Glad you asked.  If we look at our descriptive table summary below, you'll see the min and max values, and the 25th, 50th, and 75th percentiles.  These numbers encompass the entire range of values a feature takes in your dataset.  The inner quartile range is from the 25th to 75th percentile.  Therefore, to find the \"fences\" for the upper and lower outlier bounds, we need to calculate 1.5x the 25th and 75th percentile numbers, and then add and subtract them to get the outlier bounds.  Anything beyond these numbers can be thought of as a bit beyond the normal central tendencies of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41553b82-8ba6-422b-81c8-e5daae9e1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0274518-8b30-494b-a218-c14827920ac2",
   "metadata": {},
   "source": [
    "Like everything else, there are tons of different methods and ways we could go about addressing the extreme values in our data.  The plan is to discuss a few popular options for how it could be done, and then we'll settle on one that makes sense for this specific data.\n",
    "\n",
    "Here's a list of what we _could_ possibly do to our outliers, and we'll discuss them in turn.\n",
    "\n",
    "- Fix the incorrect value\n",
    "- Impute/transform the value\n",
    "- Delete the entire observation (row)\n",
    "- Delete the entire feature (column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a9e7a-510a-4bb4-a12b-c698f80eac56",
   "metadata": {},
   "source": [
    "Beyond our summaries, at this point we would definitely be ploting our features and distributions as well, but we'll mainly save that for the next section when we can cover them in greater detail.  For now we'll just look at what's know as a boxplot.  Boxplots are graphical representations of a feature's value range, highlighted by their percentiles (horizontal lines).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f06eb-1da3-4dd4-95f9-f7fc30857329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into a stacked-long format\n",
    "plt_dat = pd.melt(dat, id_vars = ['id', 'gender','country'])\n",
    "\n",
    "# Filter to only the age and minute columns & update the value column from an object to a float\n",
    "plt_dat = plt_dat[plt_dat.variable.isin(['age','active_mins','sleep_disturb_mins','sleep_rem_mins'])]\n",
    "plt_dat['value'] = plt_dat['value'].astype('float')\n",
    "\n",
    "# Plot boxplot distributions\n",
    "(gg.ggplot(plt_dat, gg.aes(x = 'factor(variable)', y = 'value', color = 'gender')) +\n",
    "     gg.xlab('') +\n",
    "     gg.geom_boxplot() +\n",
    "     gg.ggtitle('Feature Distributions - (All values)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779245a3-df25-48d8-a406-fda3be2758c4",
   "metadata": {},
   "source": [
    "As we can see above, the outliers at the top of the graph are making it nearly impossible to actually see the range of our values at the bottom of graph.  They are clearly so far out of the normal range that something is probably wrong.  Here we would start by going right back to the source with our business domain expert.  We would take our Data Quality Report and some examples and ask direct questions about how or if these values could be real.  Some of them will be obvious they're incorrect, while others may require someone with more knowledge about the topic to help.  Let's say we do so, and here's what we found out.\n",
    "\n",
    "- __Age:__ They don't know why there are nonsense ages like 0 and 146 are showing up, but they know for sure that the minimum and maximum ages in the study were 16 and 100, respectively.\n",
    "- __All Sleep Minute variables:__  The 9999 were placeholders that were never followed up on and the actual values have been lost.  We do know for a fact that no one in the study slept more than 17 hours total on average.\n",
    "\n",
    "Given this feedback, we have a little work to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78af8c-4ed1-497a-a0de-05fdaff15cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf1d04-315b-447b-9619-51b022db3829",
   "metadata": {},
   "source": [
    "We'll start with the age feature and go from there.  How many people are we talking about less than 16 and greater than 100 years of age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90f986-7c7e-4615-81f7-06dc7023963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to less than 16 or greater than 100\n",
    "age_mask = (dat.age < 16) | (dat.age > 100 )\n",
    "dat[age_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a963b-5593-453f-ab4d-12dec26b1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many observations were in our filter mask\n",
    "(age_mask == True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e388c6e-b1cc-46c0-bdc6-b99902d500c6",
   "metadata": {},
   "source": [
    "It would appear we're talking about only 12 participants with age irregularities.  We could of course remove them, but I'd rather see if we can salvage the ones that look like they have real sleep pattern minutes.  There are several with both age discrepancies _and_ clearly incorrect static values of just '2' across the board.  For these, it's just too much synthetic data if we try to impute over these values with proxies so we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c9d0b-9f56-4990-9812-bf0a806f45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where Age = 0 & all the sleep minutes = 2 by index\n",
    "dat.drop(dat[(dat.age == 0) & (dat.active_mins == 2) & (dat.sleep_disturb_mins == 2) & (dat.sleep_rem_mins == 2)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30814952-f338-4a09-9ded-233688a90ec1",
   "metadata": {},
   "source": [
    "Alright, now how many do we have left less than 16 or more than 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f98bc-9822-442a-b194-ecdc551872c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to see how many we have less than 16 or greater than 100\n",
    "dat[(dat.age < 16) | (dat.age > 100 )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a9f8d-d842-4472-ae23-b7060687aaa1",
   "metadata": {},
   "source": [
    "We'll take a simple gamble on the remaining age issues to keep from having to remove another 8 observations.  We'll assume that anything less than 16 should be the minimum, and anything over 100 should be the maximum.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12766add-0928-44c3-b1bf-ee37873d86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute for less than 16 and greater than 100\n",
    "dat.loc[dat.age < 16, 'age'] = 16\n",
    "dat.loc[dat.age > 100, 'age'] = 100\n",
    "dat[['age']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d73a85-72ff-49f3-ad44-58d6971b5349",
   "metadata": {},
   "source": [
    "Awesome!  Age looks to be all taken care of now.  Let's turn our attention to the sleep minutes.  We know that all of the 9999 placeholders are bogus and we have no expectation of collecting the real data.  So what can we do?  We can either impute or transform them in some way, or delete the observations.  Let's see how many are involved before we decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ca09a-3087-4ff2-9203-fb363337704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to all of the 9999 values\n",
    "dat[(dat.active_mins == 9999) | (dat.sleep_disturb_mins == 9999) | (dat.sleep_rem_mins == 9999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab09279-2197-464a-a2c7-a5b9dab244d0",
   "metadata": {},
   "source": [
    "These values highlight an important distinction between outliers and _extreme_ values.  Outliers are typically regarded as real data and could actually be the values you want to study.  Extreme values could possibly be real, however they are so far outside of what's considered normal for a model that's trying to learn what \"typical\" looks like.  You may want to see them, but most often we're working with models that learn \"on the averages\" what \"normal\" conditions and behaviors are.  These extreme values typically hamper that effort.  \n",
    "\n",
    "In our case, they're not even really extreme values, so much as they're completely fake data used as placeholders to indicate the researcher did not have the results when it was time to record their observations.  Because of that, there's really nothing I'd recommend we try to do to guess what they might have been.  Unfortunatley it's another case of dropping these participant observations from our set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef10d90-12d9-44e0-bd24-7332b6550816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 9999 placeholder observations\n",
    "dat.drop(dat[(dat.active_mins == 9999) | (dat.sleep_disturb_mins == 9999) | (dat.sleep_rem_mins == 9999)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804c0d7-b21e-4ad8-b333-79bccc490926",
   "metadata": {},
   "source": [
    "Looking good!  One more thing we need to check now.  Remember our business sponsor told us no one slept more than 17 hours total on average?  Let's make sure that's what we see in the data by creating a \"sleep_total_mins\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d0dfb-1ad8-4579-93c4-08ec9318d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"total\" sleep minutes variable\n",
    "dat['sleep_total_mins'] = dat[['active_mins','sleep_disturb_mins','sleep_rem_mins']].sum(axis = 'columns')\n",
    "\n",
    "# Check if any of the toal sleep minutes add up to more than 17 hours\n",
    "(dat.sleep_total_mins > 17 * 60).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57dc18-5383-4fb6-b4b4-709ec1566029",
   "metadata": {},
   "source": [
    "Whew... all good there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bec7c1-4f46-4d30-a461-1d2cb999268c",
   "metadata": {},
   "source": [
    "Now let's take a look at our boxplot and see if our distributions make a little more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e7f9a-cffb-4b3a-8b47-83c160c506b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into a stacked-long format\n",
    "plt_dat = pd.melt(dat, id_vars = ['id', 'gender','country'])\n",
    "\n",
    "# Filter to only the age and minute columns & update the value column from an object to a float\n",
    "plt_dat = plt_dat[plt_dat.variable.isin(['age','active_mins','sleep_disturb_mins','sleep_rem_mins','sleep_total_mins'])]\n",
    "plt_dat['value'] = plt_dat['value'].astype('float')\n",
    "\n",
    "# Plot boxplot distributions\n",
    "(gg.ggplot(plt_dat, gg.aes(x = 'factor(variable)', y = 'value', color = 'gender')) +\n",
    "     gg.xlab('') +\n",
    "     gg.geom_boxplot() + \n",
    "     gg.theme(axis_text_x = gg.element_text(rotation = 45, hjust = 1)) +\n",
    "     gg.ggtitle('Numeric Feature Distributions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615b1b1-3a7d-42ed-8862-afe2e9755736",
   "metadata": {},
   "source": [
    "Great!  Removing the extreme values shortened up our ranges so now we can actually see what's going on.  They way these box plots work is the dots you see are actually what we've labeled as outliers.  Those are the values past the 1.5x IQR +/- fences.  We definitely have a few remaining variables beyond those thresholds, but for now I think we'll leave them alone and deal with them when we get to the {doc}`../Chapter5/feat_engineering` and transformations section.  Hopefully they'll get smoothed out then, or we may apply what's known as a \"clamp transformation\" by imputing to the 1.5x IQR values.  Stay tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f51fea-78b9-45dc-84b3-8ab99a1d2c61",
   "metadata": {},
   "source": [
    "Lastly, don't forget that we need to handle the categorical variables which shouldn't take too long.  Let's print them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f71b5-afa2-4c22-8189-b9d0a248b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categorical summary stats\n",
    "dat[['id','gender','country','study_begin','study_end']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f4e05-aecd-4970-bf1b-a1e2fa642e54",
   "metadata": {},
   "source": [
    "Ok, looks like all we need to deal with are the dates.  We've already discussed that removing any feature which is completely static, or exactly the same for every observation, should be removed because it holds no information.  You may want to peel these off and hold them as meta data of some sort so you don't lose the information entirely, but for our analysis and modeling purposes, get them out of the way by dropping them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbe390-198e-4dc6-a4b1-f4f9b0779e6a",
   "metadata": {},
   "source": [
    "From the table above, we can see that only the dates are completely static because they only have one unqiue value count.  Get rid of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515e004-e279-4178-a400-eccf13cc0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.drop(['study_begin', 'study_end'], axis = 1, inplace = True)\n",
    "dat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573c0bc-4cf8-40ec-af7e-225e43dff748",
   "metadata": {},
   "source": [
    "And that's finally it for now!  Nicely done.  I'm sure this seemed like a ton of stuff all over the place thrown at you, but it does become more habitual the more you practice.  And usually you'll jump in and just start noticing things that look odd, start investigating, and it'll lead you right to where you need to go.  We also did most of this without the aid of plots and graphs, which are a _huge_ benefit to our EDA work.  It would have simply been too much to try and teach EDA plus plots at the same time.  This is where we head next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
